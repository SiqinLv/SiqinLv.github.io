- 问答系统有：
  - Facebook开源的Blender系统，他具有个性人物聊天的功能，可以知识问答，是有史以来最大的开放域（Open-Domain）聊天机器人。
  - 还有Uber开源的Plato系统，也都具有比较完整的功能。
  - 框架完整性，可扩展性，易用性等各方面，RASA当仁不让是当前最全面的系统之一。
- Rasa为建立高效，灵活，专有的上下文对话机器人提供了必要的基础架构和工具。使用Rasa，任何人员都可以通过文本编辑器配置配置文件，就可以得到一个非常不错的对话机器人。
- RASA跟踪了最前沿的技术，并应用到系统中，保证了RASA的技术领先性。
- RASA的NLU为开发人员提供了解消息，确定意图并捕获关键上下文信息的技术。
- 支持多种语言，单一和多种意图，以及预训练和自定义实体等功能。RASA的CORE提供了多轮对话管理机制，使用Transformer技术自动学习上下文的与当前意图的关联性，而不是比较固定的状态机，再结合RulePolicy，提供了最大的灵活性。
- RASA也提供的非常好的扩展性
  - Tokenizer
  - Featurizer
  - Classifier
  - Policy
  - Action
  - 可以只使用RASA的pipeline框架
  - 可其他各个组件都自己定义
  - 使用Rasa-x工具可视化的配置机器人
  - RASA提供了一套从开发、测试到生产部署全套的解决方案
  - Conversation-Driven Development (CDD)一套方法论
  - Action Server
  - Rasa X
  - 支持Docker部署方式，简化生产运维等方式
  - ![image](https://user-images.githubusercontent.com/70061450/171103742-51bfa5fe-2424-4631-828f-4d3b0894c964.png)
- rasa处理一条用户消息的信息流向
  - ![image](https://user-images.githubusercontent.com/70061450/171107369-cfb881c1-f508-4cd1-9f3c-afa9f60b2415.png)
  - Dialogue Transformers是rasa工程师为多轮对话管理提供的一种思路
    - 使用Transform架构管理对话，把self-attention机制应用到每一轮对话序列上
    - 常见做法是使用层次RNN去编码多轮对话，这个假定了每轮对话都与完整序列相关，当当前对话有多个话题重叠时候，就会出现偏差。
    - 而transform架构，会通过selfattention机制，适当的忽略无关的历史上文，改进了RNN的缺陷。
- 在目前多轮对话有两种方法：
  + 第一种，Dialogue Stacks，这种模型认为子对话都存在于堆栈上，其中新主题到来时候引入到堆栈上，并且主题结束后就从堆栈中弹出，然后继续堆栈中的下一个主题。
    - 简单的理解，就是类似于函数调用，调用新函数时候，CPU将旧函数信息压入到堆栈，函数执行完成后，出栈
    - 缺点1：虽然堆栈自然允许处理和结束子对话，但堆栈的严格结构也有局限性。
    - 缺点2：RavenClaw的作者主张显式地跟踪主题，以支持用户意图的上下文解释。但是，一旦从对话堆栈中弹出一个主题，它就不能再提供此上下文。
    - 缺点3：如果主题已经从堆栈中弹出，这将不再有助于澄清用户想知道什么。
    > 在对话过程中，我们不能限制人们主题交错和重新介入的这些方式，因此我们需要一种比堆栈更灵活的架构
  + 第二种：使用RNN来处理多轮对话序列，我们期望，只要有足够的训练数据，RNN应该能够学习任何期望的行为。然而，在当前没有足够的语料库的情况下，并不能保证RNN可以学习生成这些行为。Vlasov等人和Sahay等人曾对RNN的基本结构进行修改，以将这种行为的归纳偏差纳入对话策略中，以此克服RNN的不适合对话建模的一个特性，RNN使用整个输入元素序列来生成编码，除非更复杂的结构（如长短期内存（LSTM））单元被训练到足够的数据上，以明确地知道它应该“忘记”序列的一部分。
  + 第三种：transform
    - transformer架构取代了RNN作为语言模型训练的标准，通过transformer XL和GPT-2等模型，在一系列语料上都实现了低困惑度，并产生了对各种下游任务有用的表示法。
    - transformer对意外输入（例如对抗性示例）更为稳健
    - 由于self-attention预先选择了哪些标记将对编码器的当前状态起作用，transformer可以忽略序列中不具信息性（或对抗性）的token
    - 为了在每个时间步进行预测，LSTM需要更新其内部内存单元，并将此更新传播到下一个时间步。如果当前时间步的输入是意外的，内部状态会受到干扰，在下一个时间步，神经网络会遇到一个与训练过程中遇到的任何情况都不同的记忆状态。
    - transformer通过self-attention机制来解释时间历程，使每个时间步的预测相互独立。如果一个变压器接收到一个不相关的输入，它可以忽略它，只使用以前的相关输入进行预测。
    - 由于transformer在每一步都选择序列中的哪些元素来产生编码器状态，我们假设它可能是处理对话历史的一个有用的架构。会话中的话语序列可能代表多个交错的话题，而transformer的self-attention可以同时学习如何理清这些话语片段，并做出适当的回应。
    - Transformers for open-domain dialogue
      - 这些架构可以在一个大型的、多样化的数据集上进行预先训练，然后针对特定领域中的面向任务的对话进行微调
      - Dinan等人使用了类似的方法，使用transformers对对话上下文以及背景知识进行编码，以研究基于开放域的对话。
        - 他们提出的体系结构有两种形式：
          - 一种是检索模型，transformers对通过排序选择的候选响应进行编码
          - 另一种是生成模型，其中使用一个transformers作为解码器，逐项生成响应
        - 我们的解决方案和这些方法的关键区别在于，我们在语篇层面上运用self-attention，关注的是对话的顺序，而不是单个回合中的标记序列。
  + Topic disentanglement in task-oriented dialogue
    - 最近的研究试图为对话策略生成神经网络结构，以此可以在单个会话中处理交错的语篇片段。Vlasov等人引入了递归嵌入对话策略（REDP）架构
    - REDP的ablation study强调REDP性能的提高是来源两方面，一个是对话历史上的注意机制，一个是从意外用户输入中恢复的复制机制。
    - 对标准RNN结构的这种修改使对话策略能够“跳过”对话历史中的特定回合，并在意外输入前后产生相同的编码器状态
    - 在不同注意力机制中，引入masking，提高有效性。在这项工作中，并没有扩充基本的RNN架构，而是用一个transformer取代它
    - 默认情况下，RNN处理序列中的每个项目以计算编码。REDP的修改是因为并非所有的对话历史都是相关的。基于这个原因，可以进一步用self-attention来代替RNN，不需要先验假设整个序列是相关的，而是对话策略应该选择哪些历史转折点，并选择相关响应。
  + Transformer Embeding Dialog（TED）Policy
    - 大大简化了REDP的体系结构
    - 与REDP类似，我们不使用分类器来选择系统操作。我们通过联合训练每个对话状态和每个系统动作的最大相似性
    - 在推理时，将对话的当前状态与所有可能的系统动作进行比较，选出相似度最高的一个。
    - 在任务型对话检索模型的训练中也采用了类似的方法。一个步骤由几个关键部分组成。
      - 特征化：策略将用户输入、系统动作和时隙特征化。
