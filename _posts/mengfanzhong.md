- 问答系统有：
  - Facebook开源的Blender系统，他具有个性人物聊天的功能，可以知识问答，是有史以来最大的开放域（Open-Domain）聊天机器人。
  - 还有Uber开源的Plato系统，也都具有比较完整的功能。
  - 框架完整性，可扩展性，易用性等各方面，RASA当仁不让是当前最全面的系统之一。
- Rasa为建立高效，灵活，专有的上下文对话机器人提供了必要的基础架构和工具。使用Rasa，任何人员都可以通过文本编辑器配置配置文件，就可以得到一个非常不错的对话机器人。
- RASA跟踪了最前沿的技术，并应用到系统中，保证了RASA的技术领先性。
- RASA的NLU为开发人员提供了解消息，确定意图并捕获关键上下文信息的技术。
- 支持多种语言，单一和多种意图，以及预训练和自定义实体等功能。RASA的CORE提供了多轮对话管理机制，使用Transformer技术自动学习上下文的与当前意图的关联性，而不是比较固定的状态机，再结合RulePolicy，提供了最大的灵活性。
- RASA也提供的非常好的扩展性
  - Tokenizer
  - Featurizer
  - Classifier
  - Policy
  - Action
  - 可以只使用RASA的pipeline框架
  - 可其他各个组件都自己定义
  - 使用Rasa-x工具可视化的配置机器人
  - RASA提供了一套从开发、测试到生产部署全套的解决方案
  - Conversation-Driven Development (CDD)一套方法论
  - Action Server
  - Rasa X
  - 支持Docker部署方式，简化生产运维等方式
  - ![image](https://user-images.githubusercontent.com/70061450/171103742-51bfa5fe-2424-4631-828f-4d3b0894c964.png)
- rasa处理一条用户消息的信息流向
  - ![image](https://user-images.githubusercontent.com/70061450/171107369-cfb881c1-f508-4cd1-9f3c-afa9f60b2415.png)
  - Dialogue Transformers是rasa工程师为多轮对话管理提供的一种思路
    - 使用Transform架构管理对话，把self-attention机制应用到每一轮对话序列上
    - 常见做法是使用层次RNN去编码多轮对话，这个假定了每轮对话都与完整序列相关，当当前对话有多个话题重叠时候，就会出现偏差。
    - 而transform架构，会通过selfattention机制，适当的忽略无关的历史上文，改进了RNN的缺陷。
- 在目前多轮对话有两种方法：
  + 第一种，Dialogue Stacks，这种模型认为子对话都存在于堆栈上，其中新主题到来时候引入到堆栈上，并且主题结束后就从堆栈中弹出，然后继续堆栈中的下一个主题。
    - 简单的理解，就是类似于函数调用，调用新函数时候，CPU将旧函数信息压入到堆栈，函数执行完成后，出栈
    - 缺点1：虽然堆栈自然允许处理和结束子对话，但堆栈的严格结构也有局限性。
    - 缺点2：RavenClaw的作者主张显式地跟踪主题，以支持用户意图的上下文解释。但是，一旦从对话堆栈中弹出一个主题，它就不能再提供此上下文。
    - 缺点3：如果主题已经从堆栈中弹出，这将不再有助于澄清用户想知道什么。
    > 在对话过程中，我们不能限制人们主题交错和重新介入的这些方式，因此我们需要一种比堆栈更灵活的架构
  + 第二种：使用RNN来处理多轮对话序列，我们期望，只要有足够的训练数据，RNN应该能够学习任何期望的行为。然而，在当前没有足够的语料库的情况下，并不能保证RNN可以学习生成这些行为。Vlasov等人和Sahay等人曾对RNN的基本结构进行修改，以将这种行为的归纳偏差纳入对话策略中，以此克服RNN的不适合对话建模的一个特性，RNN使用整个输入元素序列来生成编码，除非更复杂的结构（如长短期内存（LSTM））单元被训练到足够的数据上，以明确地知道它应该“忘记”序列的一部分。
  + 第三种：transform
    - transformer架构取代了RNN作为语言模型训练的标准，通过transformer XL和GPT-2等模型，在一系列语料上都实现了低困惑度，并产生了对各种下游任务有用的表示法。
    - transformer对意外输入（例如对抗性示例）更为稳健
    - 由于self-attention预先选择了哪些标记将对编码器的当前状态起作用，transformer可以忽略序列中不具信息性（或对抗性）的token
    - 为了在每个时间步进行预测，LSTM需要更新其内部内存单元，并将此更新传播到下一个时间步。如果当前时间步的输入是意外的，内部状态会受到干扰，在下一个时间步，神经网络会遇到一个与训练过程中遇到的任何情况都不同的记忆状态。
    - transformer通过self-attention机制来解释时间历程，使每个时间步的预测相互独立。如果一个变压器接收到一个不相关的输入，它可以忽略它，只使用以前的相关输入进行预测。
    - 由于transformer在每一步都选择序列中的哪些元素来产生编码器状态，我们假设它可能是处理对话历史的一个有用的架构。会话中的话语序列可能代表多个交错的话题，而transformer的self-attention可以同时学习如何理清这些话语片段，并做出适当的回应。
    - Transformers for open-domain dialogue
      - 这些架构可以在一个大型的、多样化的数据集上进行预先训练，然后针对特定领域中的面向任务的对话进行微调
      - Dinan等人使用了类似的方法，使用transformers对对话上下文以及背景知识进行编码，以研究基于开放域的对话。
        - 他们提出的体系结构有两种形式：
          - 一种是检索模型，transformers对通过排序选择的候选响应进行编码
          - 另一种是生成模型，其中使用一个transformers作为解码器，逐项生成响应
        - 我们的解决方案和这些方法的关键区别在于，我们在语篇层面上运用self-attention，关注的是对话的顺序，而不是单个回合中的标记序列。
  + Topic disentanglement in task-oriented dialogue
    - 最近的研究试图为对话策略生成神经网络结构，以此可以在单个会话中处理交错的语篇片段。Vlasov等人引入了递归嵌入对话策略（REDP）架构
    - REDP的ablation study强调REDP性能的提高是来源两方面，一个是对话历史上的注意机制，一个是从意外用户输入中恢复的复制机制。
    - 对标准RNN结构的这种修改使对话策略能够“跳过”对话历史中的特定回合，并在意外输入前后产生相同的编码器状态
    - 在不同注意力机制中，引入masking，提高有效性。在这项工作中，并没有扩充基本的RNN架构，而是用一个transformer取代它
    - 默认情况下，RNN处理序列中的每个项目以计算编码。REDP的修改是因为并非所有的对话历史都是相关的。基于这个原因，可以进一步用self-attention来代替RNN，不需要先验假设整个序列是相关的，而是对话策略应该选择哪些历史转折点，并选择相关响应。
  + Transformer Embeding Dialog（TED）Policy
    - 大大简化了REDP的体系结构
    - 与REDP类似，我们不使用分类器来选择系统操作。我们通过联合训练每个对话状态和每个系统动作的最大相似性
    - 在推理时，将对话的当前状态与所有可能的系统动作进行比较，选出相似度最高的一个。
    - 在任务型对话检索模型的训练中也采用了类似的方法。一个步骤由几个关键部分组成。
      - 特征化：策略将用户输入、系统动作和时隙特征化。
      - TED策略可以是端到端的，也可以是模块化的
      - 模块化方法类似于基于POMDP的对话策略或混合代码网络
      - 使用外部自然语言理解系统，用户输入被特征化为一个二进制向量，表示识别的意图和检测到的实体
      - 对话策略从一个固定的系统动作列表中预测一个动作。系统动作的特征是以二进制向量表示动作名称，遵循REDP方法
      - “端到端”的方法指的是对话序列之外没有监督。也就是说，NLU输出或系统动作名称没有黄金标签。TED是一个end-retrieval模型，它并不生成一个新的策略响应。
      - 在端到端的设置中，用户和系统的话语被编码为词袋向量。在对话的每一步，槽总是以二进制向量的形式来表示它们的存在、不存在或值对用户重不重要。我们使用一个简单的槽跟踪方法，用最近的值覆盖每个槽。
        - ![image](https://user-images.githubusercontent.com/70061450/171138988-b5ce42b7-f330-4173-bc8a-319b44b2fe2d.png)
      - Transformer的输入是用户输入和系统动作的序列。
      - 我们利用transformer中的self-attention机制，在每轮对话中，动态地访问历史对话的不同部分。对历史对话的关联性是从数据中学习出来的，并在每轮对话中重新计算。最重要的是，它允许对话策略在这一轮对话中考虑用户的话语，而在另一个轮对话中完全忽略它。
      - Similarity
        - ![image](https://user-images.githubusercontent.com/70061450/171139970-02208fc0-ecee-4350-a1b4-2f56fbb0cba5.png)
        - 一轮对话的损失函数来自于一个向量空间，这个空间是所有负采样的汇总和每一步的损失的平均值汇总到一轮对话上。
        - 总的损失函数就是，所有的每步对话的损失函数的平均值。
        - 在推理阶段，点积相似度就作为下一个对话的检索问题
        - 在模块化训练中，我们使用平衡批处理策略来减轻不同类别的不平衡，因为有些系统动作要比其他动作频繁得多。
        - 训练模型的训练对话量和测试集中每个动作都被正确预测的对话数
          - 同时满足我们两个标准的对话数据集是REDP数据集、MultiWOZ和Google Taskmaster-1,对于后者，我们必须从实体注释中提取操作标签，这并不总是可行。在我们的实验中，两个不同的模型作为基线。
          - Vlasov等人提出的REDP模型是专门为处理长期历史依赖而设计的，但它是基于LSTM的。
          - 另一个基于LSTM的策略与TED相同，只是transformer被LSTM替换了。
          - REDP模型是专门为处理长期历史依赖而设计的，但它是基于LSTM的
          - TED策略的性能与REDP不相上下，没有任何专门设计的体系结构来解决任务，并且显著优于基于LSTM的简单策略。在极端低数据环境下，TED政策的表现优于REDP。
          - REDP在非合作性对话以后，严重依赖其复制机制来预测先前提出的问题。然而，TED策略既简单又通用，在不依赖对话属性（如重复问题）的情况下也能实现类似的性能。
          - 由于transformer架构，TED策略的训练速度比REDP快，并且需要更少的训练时间来达到同样的精度。
          - TED政策从历史中选择与当前预测相关的关键对话步骤，而忽略了不具信息性的历史对话。在这里，我们只看到一个对话，但是对于任意数量的闲聊对话，结果是相同的。
          - 在证明了轻量级TED策略的性能至少与专用REDP不相上下，并且在对包含长期历史依赖的会话进行评估时显著优于基本LSTM策略
          - 模型评估依据使用max history N.All scores的MultiWOZ 2.1数据集
        - transformer embedding dialogue（TED）策略
          - transformer 的self-attention机制在对话轮次的顺序上运行。这是一个比RNN更合适的体系结构，因为在现实生活中存在交叉主题
          - 我们证明了TED策略可以以模块化和端到端的方式应用于MultiWOZ数据集，尽管我们也发现，由于缺乏历史依赖性和对个体群体工作者偏好的依赖性，该数据集不适合用于对话策略的监督学习。我们还对一个面向任务的数据集进行了实验，专门用来测试从非合作用户行为中恢复的能力。
          - TED更快、更简单、更通用，TED policy的性能优于基线LSTM方法，并与REDP不相上下。
          - 我们证明学习的注意权重很容易解释，并且反映了对话逻辑。在每一个对话回合，一个transformer选择前几轮对话，给出当前预测，有选择地忽略或关注对话历史的不同轮次。
  - RASA整体就是pipeline结构
    - NLU(自然语言理解)
      - NLU模块也是一个可细分pipeline结构
        - Tokenize（标记解析、令牌化）
          - Tokenize空格分词或结巴分词
        - Featurize（特征）
          - 特征向量使用One-hot编码或者CountVectorsFeaturizer编码
        - NER Extract（实体抽取）
        - Intent Classify（意图分类）
      - 预训练的语言模型
        - MITIENLP（下载：https://github.com/mit-nlp/MITI）
          - 配置：
            ```python
              pipeline:
                - name: "MitieNLP"
                model: "data/total_word_feature_extractor.dat"
            ```
          - mitie 基于dlib库开发，dlib是一个c++高性能机器学习库。所以对性能有要求的信息抽取场景，可以考虑使用mitie。mitie现有的资料比较少，github最近更新也是五六年前了。从mitie代码中看到它的NER使用structural_sequence_labeling_trainer.实现的细节见https://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf文中指出，MITIE的NER是HMM 和SVM相结合做的。相比单纯的HMM，这种方法是基于最大margin 标准。这相比纯CRF或者最大熵的HMM有很多优势：
          - 可以通过核函数学习非线性的判断关系
          - 可以处理overlapping features.
          - 优点：但MITIE有无可比拟的速度优势，在算力敏感的情况下自己权衡选择，RASA已经不做官方推荐了。
          - 缺点：但毕竟是基于传统机器学习的方式，相对于BERT这种海量语料预训练模型来说，效果还是稍差一点，这个可以使用RASA的NLU评估工具跑分试一下
          - 另外，MITIE没有预训练的中文模型，如果想开发中文机器人，需要自己训练语言模型。具体参考用Rasa NLU构建自己的中文NLU系统。
      - SpaCyNLP（下载：https://github.com/explosion/spacy-models or python -m spacy download zh_core_web_sm）
        - 配置:
          ```python
            pipeline:
              - name: "SpacyNLP"
              model: "zh_core_web_sm"
          ```
        - spaCy是一个用Python和Cython编写的高级自然语言处理的库,它跟踪最新的研究成果，并将其应用到实际产品。
        - spaCy带有预训练的统计模型和单词向量，目前支持60多种语言
        - 它用于标记任务，解析任务和命名实体识别任务的卷积神经网络模型，在非常快速的情况下，达到比较好的效果，并且易于在产品中集成应用。
        - 最新发布的spaCy3.0也集成了Transformer相关模型。
        - 使用spaCy时，文本字符串的第一步是将其传递给NLP对象。
        - NLP对象本质上是由几个文本预处理操作组成的管道，输入文本字符串通过管道后，最终输出文档，完成各种功能。
          ![image](https://user-images.githubusercontent.com/70061450/171316259-69de49c4-e4fa-438f-8a59-d08ddcd9cb24.png)
        - 以前还有一个HFTransformersNLP，里面主要包含一些BERT，GTP等比较新的Transformer模型，现在已经在LanguageModelFeaturizer中实现了，不在需要在pipeline里面配置HFTransformersNLP语言模型。

    - DST(对话状态追踪,作用是根据领域(domain)/意图(intention) 、槽值对(slot-value pairs)、之前的状态以及之前系统的Action等来追踪当前状态)
    - DPL(策略学习)
    - 执行一个动作
- Dual Intent and Entity Transformer——RASA论文翻译
  - 大规模预训练语言模型在GLUE和SuperGLUE等在语言理解任务上取得非常好的效果，相比其他预训练方法如分布式表示（GloVe）和纯监督方法都有显着改善。
  - 我们引入双重意图和实体 Transformer（DIET\[Dual Intent and Entity Transformer\]一种多任务transformer 架构）架构，并研究不同的预训练表示在语义理解意图识别任务和实体预测任务上是否有效。
  - DIET 在复杂的多领域NLU数据集取得state of art的效果，并在其它更简单的数据集上取得很好的性能。
  - DIET甚至在没有任何预训练词嵌入的情况下也可以取得state of art的结果
  - 性能最好的模型优于微调的 BERT，训练速度快大约六倍。
  - 数据驱动对话建模的两种常见方法是端到端和模块化系统。
  - 模块化方法如基于 POMDP（部分可观测马尔科夫决策过程） 的对话策略和 Hybrid Code Networks(混合代码网络)使用独立的自然语言理解（NLU）和生成（NLG）系统。
  - 对话策略接收 NLU 系统的输出，选择下一个系统操作，然后在 NLG 系统生成对应的响应。
  - 在端到端方法中，用户信息直接输入到对话策略中预测下一个系统语句。 
  - 在对话系统的上下文中，自然语言理解通常指两个子任务：意图分类和实体识别
  - 最近的工作表明，经过预先训练的大型语言模型在具有挑战性的语言理解基准方面产生最佳性能。 但是，这类模型的预训练和微调的计算成本都相当可观
  - 对话系统不仅由研究人员开发，而且由全球成千上万的软件开发人员开发。 仅 Facebook 的 Messenger 平台就支持数十万个第三方对话助手。 对于这些应用程序，希望可以对模型进行快速训练和迭代以适合典型的软件开发工作流程。 此外，由于许多这些助手中都使用英语以外的其它语言，因此重要的是要了解无需 大规模的训练能达到什么样的性能。
  - DIET（Dual Intent and Entity Transformer），这是一种用于意图分类和实体识别的新型多任务体系结构。一个关键的特性是能够以即插即用的方式结合语言模型的预训练单词嵌入，并将它们与单词和字符级 n-gram 稀疏特征结合起来。 
  - 我们的实验表明，即使没有预训练的嵌入，仅使用单词和字符级 n-gram 稀疏特征，DIET 仍可以在复杂 NLU 数据集上取得state of art的结果。 此外，添加预训练语言模型的单词和句子嵌入，可进一步提高所有任务的整体准确性。 我们性能最好的模型明显优于fine-tune的 BERT，训练速度快六倍。
  - 稠密表示的迁移学习
    - 在 GLUE和 SuperGLUE等语言理解基准上表现优异的模型得益于使用大型预训练语言模型的单词和句子的密集表示，如 ELMo、BERT、GPT等。由于这些嵌入是在大规模自然语言文本语料库上进行训练的，因此无论是否进行微调，它们都可以很好地泛化到各个任务，并且可以作为输入特征迁移移到其它语言理解任务上. 不同的微调策略被提出来实现跨任务的有效迁移学习
    - 对 BERT 这样的大型预训练语言模型进行微调可能并非对每个下游任务都是最佳的。 而且，这些大规模语言模型速度慢、训练成本高，因此对于现实世界中的对话 AI 应用而言并不理想
    - 为了获得更紧凑的模型，在 Reddit 的大型会话语料库上对单词和句子级别的编码器进行预训练将获得句子级别的密集表示（无需微调）迁移到意图分类的下游任务，其性能要好于 BERT 和 ELMo 的嵌入。
  - 联合意图分类和命名实体识别
    - 研究了许多方法，多任务设置中的训练意图分类和命名实体识别（NER）。
    - 提出一种由双向门控循环单元（BiGRU）组成的联合体系结构，每个时间步骤的隐藏状态用于实体标记，最后一个时间步骤的隐藏状态用于意图分类。 
    - 提出一种基于注意力的双向长期短期记忆（BiLSTM），用于联合意图分类和 NER。
    - 在每个意图和实体关注单元的上方引入一个共同关注网络，用于每个任务之间的相互信息共享。 
    - 提出联合 BERT，在 BERT 之上并以端到端的方式训练，他们将第一个特殊标记 [CLS] 的隐藏状态用于意图分类，实体标签使用其它词符的最终隐藏状态预测。
    - 提出一种分层的自下而上的体系结构由，用 BiLSTM 单元组成捕获语义框架的较浅表示，他们从以自下而上的方式堆叠的各个层学到的表示形式来预测对话行为、意图和实体标签。
    - 我们的 DIET采用类似的基于 transformer 的多任务设置，并且还进行融合研究以观察与其他单任务架构相比的有效性。
    - DIET体系架构：![image](https://user-images.githubusercontent.com/70061450/171334566-c79b13e3-74ff-49e6-bd33-ec882ce0f404.png)
    - ![image](https://user-images.githubusercontent.com/70061450/172126403-69bf0c81-e0d4-4d1f-96dc-2668b72061ab.png)
    - 数据集：
      - NLU-Benchmark 数据集 NLU-Benchmark 数据集，可在线访问，带有场景、动作和实体的标注
      - ATIS ATIS（Hemphill 等人，1990）是 NLU 领域中经过充分研究的数据集。 它由预订机票的人的录音经过标注转录。
      - SNIPS 此数据集是从 Snips 个人语音助手收集的
    - 我们的模型用 Tensorflow 实现。 我们使用 NLU-Benchmark 数据集的第一小份来选择超参数。使用 Adam进行优化，初始学习率为 0.001。
    - NLU-Benchmark 数据集包含 10 个小份，每个小份具有单独的训练和测试集。 
    - HERMIT和我们在 NLU-Benchmark 数据集上表现最好的 DIET 配置的结果。 我们性能最好的模型使用单词和字符级别的稀疏特征，并将它们与 ConveRT 的嵌入结合在一起。 该模型不使用屏蔽损耗（由†表示）。
    - 在 NLU-Benchmark 数据集上表现最好的模型的结果。 我们性能最好的模型使用稀疏特征，即词符级别的 one-hot 编码和字符 n-gram 的 multi-hot 编码（n ≤ 5）。 这些稀疏特征与 ConveRT 的密集嵌入相结合
    - 们性能最好的模型没有使用屏蔽损失（在第3节描述，在表中由†表示）。 我们在意图方面的表现优于 HERMIT，绝对值超过 2％。 我们的实体 F1 微观平均得分（86.04％）也高于 HERMIT（84.74％）。 HERMIT 报告的实体精度值相似，但是，我们的召回率要高得多（86.13％ 相比 82.04％）。
    - 在 NLU-Benchmark 数据集上仅对一项任务进行 DIET 训练，即意图分类或实体识别。
  - 联合训练的重要性
    - 为了评估意图分类和命名实体识别这两个任务是否受益于联合优化，我们针对每个任务分别训练了模型
    - 使用 DIET 仅训练单个任务的结果。 结果表明，与实体识别一起训练时，意图分类的性能略有下降
    - 意图分类单任务训练的最佳性能配置对应于使用没有 transformer 层5 的 ConveRT 嵌入。当单独训练实体时，实体的 micro-averaged F1 分数从 86.04 ％下降到 82.57％。 检查 NLU-Benchmark 数据集，这可能是由于特定意图与特定实体的存在之间的强相关性。
    - 不同特征组件和屏蔽的重要性
      - 不同预训练语言模型的嵌入都可以用作密集特征。

          
      
